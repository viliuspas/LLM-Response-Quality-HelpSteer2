{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9160e874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np, torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (AutoTokenizer, AutoModel, DataCollatorWithPadding)\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd89b52",
   "metadata": {},
   "source": [
    "### Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbd56190",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568024e0",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4e3cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"nvidia/HelpSteer2\")\n",
    "train_data, val_data = ds[\"train\"], ds[\"validation\"]\n",
    "\n",
    "label_cols = [\"helpfulness\", \"correctness\", \"coherence\", \"complexity\", \"verbosity\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7cc764",
   "metadata": {},
   "source": [
    "### Tokeniser & encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064df24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tok = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ffb3ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tok_pair(batch):\n",
    "    return tok(batch[\"prompt\"], batch[\"response\"], truncation=True, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3ddf221",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47fb470490e54db188335817d7ab9412",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1038 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.map(tok_pair, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
    "val_data   = val_data.map(tok_pair, batched=True, remove_columns=[\"prompt\", \"response\"])\n",
    "\n",
    "train_data.set_format(type=\"torch\",\n",
    "                      columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"] + label_cols,\n",
    "                      output_all_columns=True)\n",
    "val_data.set_format(type=\"torch\",\n",
    "                    columns=[\"input_ids\", \"attention_mask\", \"token_type_ids\"] + label_cols,\n",
    "                    output_all_columns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e7492",
   "metadata": {},
   "source": [
    "### Custom collate with dynamic padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc67113",
   "metadata": {},
   "outputs": [],
   "source": [
    "padder = DataCollatorWithPadding(tok, return_tensors=\"pt\")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    features = [{k: v for k, v in item.items() if k not in label_cols}\n",
    "                for item in batch]\n",
    "    batch_padded = padder(features)\n",
    "    for attr in label_cols:\n",
    "        batch_padded[attr] = torch.tensor([item[attr] for item in batch],\n",
    "                                          dtype=torch.long)\n",
    "    return batch_padded\n",
    "\n",
    "batch_size = 8\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader   = DataLoader(val_data, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5a2cd",
   "metadata": {},
   "source": [
    "### Multi-head model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce12f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadReward(nn.Module):\n",
    "  def __init__(self, enc_name):\n",
    "      super().__init__()\n",
    "      self.enc = AutoModel.from_pretrained(enc_name)\n",
    "      h = self.enc.config.hidden_size\n",
    "      \n",
    "      self.layer_norm = nn.LayerNorm(h)\n",
    "      self.dropout1 = nn.Dropout(0.3)  # Reduced first dropout\n",
    "      self.dropout2 = nn.Dropout(0.5)  # Keep high dropout before final layer\n",
    "      \n",
    "      self.intermediate = nn.Linear(h, h // 2)\n",
    "      self.heads = nn.ModuleList([nn.Linear(h // 2, 5) for _ in range(5)])\n",
    "      \n",
    "      for head in self.heads:\n",
    "          nn.init.xavier_uniform_(head.weight)\n",
    "          nn.init.zeros_(head.bias)\n",
    "      nn.init.xavier_uniform_(self.intermediate.weight)\n",
    "      nn.init.zeros_(self.intermediate.bias)\n",
    "      \n",
    "  def forward(self, **enc_inputs):\n",
    "      out = self.enc(**enc_inputs).last_hidden_state[:, 0]  # [CLS]\n",
    "      out = self.layer_norm(out)\n",
    "      out = self.dropout1(out)\n",
    "      out = torch.relu(self.intermediate(out))\n",
    "      out = self.dropout2(out)\n",
    "      return [head(out) for head in self.heads]  # (B, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f91269c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941ac6f2",
   "metadata": {},
   "source": [
    "### Test results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8fb4895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, 'models/metric_model_6.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c078725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadReward(model_name).to(device)\n",
    "model.load_state_dict(torch.load(r'models\\improved2\\best_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b13096b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(prompt, response):\n",
    "    model.eval()\n",
    "    inputs = tok(prompt, response, return_tensors=\"pt\", truncation=True, max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    preds = [torch.softmax(out, dim=-1).argmax().item() for out in outputs]\n",
    "    return dict(zip(label_cols, preds))\n",
    "  \n",
    "def print_load_data(dataset, idx):\n",
    "    print(f'Actual:\\n{json.dumps(dataset[idx], indent=2)}\\n')\n",
    "    print(f'Pred:\\n{predict(dataset['prompt'][idx], dataset['response'][idx])}')\n",
    "    \n",
    "def calculate_score_accuracy(dataset, label_cols):\n",
    "    correct = {col: 0 for col in label_cols}\n",
    "    total = {col: 0 for col in label_cols}\n",
    "\n",
    "    for idx in tqdm(range(len(dataset)), desc=\"Calculating Accuracy\"):\n",
    "        preds = predict(dataset['prompt'][idx], dataset['response'][idx])\n",
    "        for col in label_cols:\n",
    "            if preds[col] == dataset[col][idx]:\n",
    "                correct[col] += 1\n",
    "            total[col] += 1\n",
    "\n",
    "    accuracy = {col: round(correct[col] / total[col], 4) if total[col] > 0 else 0 for col in label_cols}\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c0584ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual:\n",
      "{\n",
      "  \"prompt\": \"explain master slave replication nsql\",\n",
      "  \"response\": \"In the context of NoSQL databases, master-slave replication refers to a configuration where a single master node writes data, and one or more slave nodes read data from the master and replicate it to provide read scalability. The master node is responsible for accepting write requests and updating its own data, while the slave nodes are responsible for replicating the data from the master and serving read requests.\\n\\nIn this configuration, the master node is the only node that can make changes to the data, while the slave nodes can only read the data and replicate it. This ensures that the data is consistent across all nodes and that there is no data loss in case of a failure of a single node.\\n\\nMaster-slave replication in NoSQL databases can be implemented using various technologies, such as Apache Cassandra, MongoDB, and Apache HBase. In each of these databases, the master-slave replication configuration can be set up using specific configuration options and parameters.\\n\\nIt's worth noting that master-slave replication is not a failover solution, as the failure of the master node will result in the loss of data until the node is brought back online. Therefore, it's important to have a proper disaster recovery plan in place to ensure that data is protected in case of a failure.\",\n",
      "  \"helpfulness\": 4,\n",
      "  \"correctness\": 4,\n",
      "  \"coherence\": 4,\n",
      "  \"complexity\": 3,\n",
      "  \"verbosity\": 2\n",
      "}\n",
      "\n",
      "Pred:\n",
      "{'helpfulness': 4, 'correctness': 4, 'coherence': 4, 'complexity': 2, 'verbosity': 2}\n"
     ]
    }
   ],
   "source": [
    "# print one prediction\n",
    "print_load_data(ds['validation'], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2f65ae3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating Accuracy: 100%|██████████| 1038/1038 [00:25<00:00, 41.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'helpfulness': 0.3815, 'correctness': 0.4778, 'coherence': 0.7254, 'complexity': 0.6127, 'verbosity': 0.6744}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# validate prediction accuracy\n",
    "print(calculate_score_accuracy(ds['validation'], label_cols))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
